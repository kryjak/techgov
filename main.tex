% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\ifLuaTeX
  \usepackage{luacolor}
  \usepackage[soul]{lua-ul}
\else
  \usepackage{soul}
\fi
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\PassOptionsToPackage{round}{natbib}
\usepackage[preprint]{neurips_2024}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{fvextra}  % extends fancyvrb
\DefineVerbatimEnvironment{PromptBlock}{Verbatim}{
  breaklines=true,
  breakanywhere=true,
  breaksymbolleft={},    % no wrap symbol in the margin
  breaksymbolsepleft=0pt,
  fontsize=\small
}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{bookmark}       % hyperlinks
\usepackage{natbib}
\usepackage{url}            % simple URL typesetting
\def\UrlBreaks{\do\/\do-\do_\do.\do=\do?\do&}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage[table,xcdraw]{xcolor}
\usepackage{csquotes}
\usepackage{svg}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{glossaries}
\usepackage{float}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{customgreen}{rgb}{0.094, 0.502, 0.22}
\usepackage{graphicx}
\usepackage{enumitem}
% \usepackage{authblk}
\usepackage{makecell,array}
\usepackage{longtable}
\usepackage[capitalise]{cleveref}
%\usepackage[backend=biber,style=apa]{biblatex} % pick your style
%\addbibresource{references.bib}
%\usepackage[hidelinks]{hyperref} % provides \
\usepackage{hyperref}
\usepackage[toc,page]{appendix}

% tell cleverref to reference Appendices as Appendices and not as Sections
\crefname{appendix}{Appendix}{Appendices} 
\Crefname{appendix}{Appendix}{Appendices}


% \bibliographystyle{abbrvnat}     % or abbrvnat, unsrtnat, etc.
\renewcommand\theadfont{\bfseries}
\renewcommand\theadalign{cc}
%\usepackage[numbers]{natbib}  
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    breaklines=true
}
\lstset{style=mystyle}

% --- Robust 4th level in NeurIPS (no duplicates) ---
\makeatletter
\AtBeginDocument{%
  % Number down to level 4 (and include in ToC if you want)
  \setcounter{secnumdepth}{4}%
  \setcounter{tocdepth}{4}%

  % Number \paragraph like 1.1.1.1
  \renewcommand\theparagraph{\thesubsubsection.\arabic{paragraph}}%

  % Make \paragraph a block-style heading (line break; not run-in)
  \renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
    {1.5ex \@plus 0.5ex \@minus 0.2ex}% space above
    {1.0ex}%                          % positive after-skip => new line
    {\normalsize\bf}}%

  % Alias: use \paragraph as the implementation of \subsubsubsection
  \let\subsubsubsection\paragraph

  % ToC entry for \subsubsubsection same as \paragraph
  \providecommand{\l@subsubsubsection}{\l@paragraph}%
}%
\makeatother

\title{Fingerprinting All AI Cluster I/O Without Mutually Trusted Processors}

\author{%
    Naci Cankaya \\ MATS Fellowship \And Jakub Kryś \\ SaferAI \And Jonathan Ng \\ Independent \And Luke Marks \\ Martian \And Felix Krückel \\ RWTH Aachen University
}

\raggedbottom

\begin{document}

\maketitle

\section{Introduction}\label{introduction}
The rapid progress of Artificial Intelligence (AI) in recent years has opened the door to accelerating advancements in multiple fields including medicine, engineering or computer science. However, as a very powerful general-purpose tool, AI also exacerbates existing concerns around technology misuse, as well as introduces novel risk vectors such as the threat of autonomous misaligned agents~\citep{internationalaisafety2026}. For this reason, several jurisdictions have already introduced legally-binding regulations governing the acceptable uses, transparency obligations or testing requirements of generative AI systems~\citep{interim_measures_cac_2023, euaiact2024, sb53_2025}. Additionally, in the future the need and political will for treaties and agreements might also arise between jurisdictions. For example, in recognition of the tremendous power afforded by AI, leading actors in this technology competition could agree on `red lines' that neither of them should cross~\citep{airedlines2025}. These may emulate historical agreements in other safety-critical areas such as The Nuclear Non-Proliferation Treaty or START.

Regardless of the nature of such legislation or treaties, there is a clear need for the ability to verify compliance with the rules by all affected parties. This problem is particularly important and challenging in the context of AI, since computing power as a resource has a much more dual-use nature than fissile materials or even DNA synthesis tools~\citep{sastry2024}. Consequently, the field of AI Verification has recently emerged as a maturing research agenda spanning both technical and governance aspects. The importance of robust verification mechanisms is reflected by the appearance in literature of three major works detailing open problems, policy goals and possible research directions in this area~\citep{miri_verification, aigi_verification, rand_verification}. These are presented concisely in~\cref{fig:verification_subgoals}.

\begin{figure}[t]
	\includegraphics[width=\linewidth]{media/verification_subgoals.png}
	\caption{A breakdown of the AI Verification research agenda into its goals and subgoals. Reproduced from~\citet{rand_verification} with authors' permission. In this work, we focus on verification Subgoal~2A.}
	\label{fig:verification_subgoals}
\end{figure}

In this work, we contribute to Subgoal 2A, which can be summarised as follows. An entity operating a computing cluster --- the Prover --- declares to the party imposing regulations --- the Verifier --- that they intend to execute certain workloads on this cluster that are compliant with such rules. The Verifier wishes to ensure that the Prover is not executing other, that is undeclared, workloads on this cluster. In other words, they verify the \textit{comprehensiveness} of the declared cluster usage. Crucially, we do not tackle the problem of determining which workloads should be allowed or not in the first place (\textit{regulation}) or of verifying whether the declared workloads are allowed (\textit{compliance}, Goal~1). \textbf{Our scope is exclusively to verify that the declared workloads constitute all the workloads that have been actually executed, regardless of whether they are compliant or not.} We also do not consider the threat vector of hidden compute clusters (Subgoal~2B).

Most solutions for verifying compliance and comprehensiveness proposed so far focus on mechanisms that can be fitted on or around hardware accelerators~\citep{onchip2024}. These include devices such as Trusted Execution Environments, HBM-based chiplets that monitor traffic in and out of the memory stacks~\citep{petrie2025guaranteeable} and tamper-proof enclosures to house the accelerators~\citep{tampersec_2026} \footnote{For a thorough overview of possible directions, we refer the reader to the three-part series by the FlexHEG initiative~\citep{flexheg2025}.}. Instead of targetting individual accelerators, in this paper we operate at the level of the whole data centre. In particular, we focus on the communication chokepoint between that hardware and the Prover: the datacenter north-south internet uplink. If the Verifier can capture, hash and timestamp
%\footnote{For security reasons, timestamps can be computed by the verifier and prover upon receiving hashes, rather than inside the border patrol device. Those timestamps are helpful for retrieval from large storage systems later on.}
\emph{all} traffic in and out of the datacenter, and the Prover can --- upon later request --- demonstrate that their declared workloads correspond to this hashed traffic, the Verifier can be satisfied that the Prover is not performing undeclared computations in the cluster. To achieve this goal, we propose a schematic design of a `Border Patrol' device that uses passive network taps \textcolor{red}{[INSERT REFERENCE]} to capture and process such traffic for later verification in a mutually trusted manner.

\textbf{Main contributions}
\begin{enumerate}
	\item A detailed description of the problem statement and the design requirement for a verification mechanism meant to address Subgoal 2A of~\citep{rand_verification} (\cref{sec:problem}).
	\item An actionable design specification of our proposed Border Patrol device that addresses these requirements. This includes a detailed description of how the device processes captures traffic in order to eliminate steganographic degrees of freedom in egress data (\cref{methods}).
	\item A feasibility assessment of this solution, including estimating the storage requirements needed for the associated verification protocol (\cref{precedents}).
	\item A demonstration of the intended functioning of the device using simulated I/O data centre traffic (\cref{results}).
\end{enumerate}

\section{Related Work}\label{related-work}
We draw from research by~\citet{fisk_active_warden} and~\citet{xing_netwarden} on active wardens for erasing steganography channels in protocol headers, as well as~\citet{lee_phy_covert_channels} and~\citet{uttatwar_active_warden} for erasing timing side-channels by adding jitter in a \textcolor{red}{bump-in-the-wire <- what is this?}. Both methods are part of our proposed Border Patrol device. While replay verification is not part of our project scope, we refer to~\citet{karvonen2025arxiv} and~\citet{cankaya_misreporting} for highlighting the importance of this downstream challenge for closing off exploitable degrees of freedom~\citep{rinberg2025arxiv} in the replay's inaccuracy if not mitigated.
\textcolor{red}{THIS NEEDS TO BE EXPANDED, AT LEAST WITH MORE EXPLANATION. CURRENTLY IT'S SUPER TECHNICAL. ALSO NEED TO TALK ABOUT: NETWORK TAPS, STEGANOGRAPHY, SIDE CHANNEL ATTACKS, TIMING ATTACKS, ETC.}

\section{Problem statement}\label{sec:problem}
We are interested in designing a device which allows the Prover to demonstrate the \textit{comprehensiveness} of its declared workloads to the Verifier. This introduces the following requirements:
\begin{itemize}
	\item The device needs to capture all of the ingress/egress traffic of a data centre.
	\item It must be mutually trusted by both Prover and the Verifier. This means that:
	\begin{itemize}
		\item The Prover must not have the ability to hide significant amounts of information in the traffic being monitored. Otherwise, they could transfer undeclared workloads out of the data centre using steganographic techniques, side-channels, timing attacks, and similar.
		\item The Verifier must not have the ability to extracts Prover's proprietary information through the device.
	\end{itemize}
	\item The solution should be retrofittable to realistic, full-scale deployments at the scale of $\mathcal{O}$(100k) accelerators. To the extent possible, it should rely on existing and affordable technologies.
	\item The device should introduce as little additional latency as possible in order to minimise the negative effect on usual data centre operations. It should be able to process millions of communication packets streaming in/out every second.
\end{itemize}

\textbf{We state our working assumptions here:}

\begin{itemize}
	\item
	Input steganography via payload data (tokens etc.) is unconstrained.\footnote{A warden could -- in principle -- rewrite prompts and input media files while preserving semantics. We leave
		this open for future research.} However, the output payload's strict causal dependence on the \textbf{declared} inputs is verified post-hoc, based on the evidence captured by the border patrol device. Only payload data is hashed, as protocol header nonces would otherwise need to be stored long-term\footnote{Otherwise, the prover could not satisfy challenges of hashes that were computed with those protocol header nonces (e.g. ISN) included.}.
	\item
	The Prover needs to store plaintext data long enough for it to be available for verification later.\footnote{This includes protocol layer 7 (application) payload data, if the hashes are only computed at those.}
	\item
	Most traffic is API (customer service, language models). Most API traffic is egress.\footnote{Because of json packaging~\citep{web_platformopenaicom} around each streamed token (worst case). Another reason why we focus on egress is working assumption 1.}
\end{itemize}

\textbf{Out of scope}

In order to convey the nuance of the problems we are --and are not--tackling, we briefly explain the ``verification pipeline'' we are assuming:
\begin{figure}[htbp]
    \centering
    \vspace{-1em}  % Pulls the image closer to the text above
    \includegraphics[width=1\textwidth]{media/image1.png}
    \vspace{-1em}  % Pulls the text below closer to the image
    \caption{Your caption here}
    \label{fig:levels}
\end{figure}
We are specifically aiming for level 2: Capturing raw evidence of computation (only hashes, not plaintext). The hashes, together with the plaintext data it points to, would later feed into a secure cluster trusted by the verifier, without the plaintext information being shown to them directly.

The verification in said cluster is twofold, and out of scope for us: replaying observed inputs and outputs to confirm they match precisely\footnote{Same as previous footnote.} (level 3), and evaluating the computations for legal compliance (level 4).

Also out of scope of this work is the physical monitoring of both the prover's cluster and the verification facility needed to prevent secret communication \emph{devices}. Finally, we do not go into detail on steganographic degrees of freedom in unverifiable information contained in application layer payloads (e.g. a limit on GPU temperature diagnostics leaving the facility). These degrees of freedom are not for our border patrol device to erase, but for rules to constrain\footnote{Since this information is hashed, compliance checking here is trivial.}.

\section{Methods}\label{methods}
\textcolor{red}{THIS NEEDS A MUCH LONGER INTRODUCTION COVERING EACH COMPONENT, ITS FUNCTION AND WHY ITS NECESSARY}
We recommend using our interactive demonstration~\citep{web_felix_krueckelcom_fingerprinting_ai_cl} of how our evidence capture device (``border patrol'') works. In addition, we give a more detailed treatment of our method and setting below.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{media/image3.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}
\textbf{The most important points about the macroarchitecture (placement of the border patrol device):}

\begin{enumerate}
\item
  The facility is under physical monitoring, ensuring that any traffic in and out of the cluster needs to pass through the border patrol chokepoint.
\item
  Every bit of traffic sent to the verifier (hashes only) can be passively observed by the prover. This places the root of trust in \emph{physics}: Beam splitting via FBT or PLC~\citep{web_enwikipediaorg_fiber_optic_splitter} (glass touching glass) and optical isolators~\citep{web_enwikipediaorg_optical_isolator_w} (one-way-street) ensure the prover can observe and double-check that hashes have been computed exactly as they expect, without being able to interfere with the verifier's monitor leg. Timestamps can be computed by both parties upon receiving the hashes at practically the
  same time.Precedents-and-Available-Hardware
\item For unfamiliar readers, TLS is the encryption between the outside world and the datacenter. Unencrypted communication within datacenters is not unusual~\citep{web_learnmicrosoftcom_enabling_end_to_end_}. With key escrows, or retroactive zero-knowledge proofs, or similar, we expect the proposed setup to also be adaptable for working with encrypted traffic, but here we leave this out of scope for simplicity.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{media/image2.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}

\textbf{The most important points about the microarchitecture (the
border patrol device itself):}

\begin{enumerate}
\item
  Here we also illustrate the second functionality of the border patrol device: An active warden \textbf{without supply chain trust}. While Fisk et al.~\citep{web_digitalcommonsmemphisedu_eliminating_steganog} and Xing et al.~\citep{pdf_sec20_xing} have demonstrated the elimination of steganography channels in protocol headers first in software, then in hardware, both assume single-party trust in the device. In contrast, we combine a coin flip protocol~\citep{web_coinfliptoolcom_cryptographic_coin_f} and passive observability of the warden's behaviour to \emph{eliminate} the requirement of \emph{mutual} trust in \emph{any} device\footnote{Why is trusted RNG required for the warden? The answer is that some network protocol headers need to include random nonces (e.g. ISNs~\citep{web_felix_krueckelcom_warden_protocol_sta} shown at L3) for security reasons, and predictability would break this. The warden overwriting these nonces may exfiltrate information in service of the prover or verifier if not mutually observed or trusted.}\footnote{To prevent latency penalties, random nonces are not generated on-demand. Instead, the coin-flip protocol populates a pre-computed entropy buffer. The active warden consumes these numbers in a strict First-In-First-Out (FIFO) order to overwrite protocol headers at line rate.}.
\item
  In the illustration above, we colour-code the \emph{minimum} device trust assumptions. This does not mean that transparent design and supply chains could not \emph{also} establish mutual trust in components, which would further add defense in depth.\footnote{The information path between the verifier and the hashing processor in particular is precarious if the processor is controlled by only them. While the prover \emph{can} observe and double-check this information, photonic or timing modulation on the outputs themselves may be dangerous channels. In appendix E, we present an extended version of the border patrol device designed to defend against such channels.}
\item
  Re-timing eliminates both analogue side channels (e.g. light modulation in optical fibre) as well as timing modulation at any relevant bandwidth. Re-timing is performed by the Active Warden~\citep{web_epj_conferencesorg_delay_normalization_} device by buffering all incoming packets (e.g. for 10ms) and then sending them out in their original order at maximum bandwidth to combat attacks such as PHY-Covert Channels~\citep{pdf_nsdi14_paper_lee}.
\end{enumerate}

We present an enhanced design in the \cref{appendix-e-enhanced-design-for-the-border-patrol-device}.

\section{Precedents and Available Hardware}\label{precedents}

While our proposal is conceptual and tested in simulation rather than real-world conditions, we emphasize that all of the individual components\footnote{With the possible exception of the coin flip devices. Since only unilateral trust is required, off-the-shelf CPUs, perhaps TEEs should suffice.} for the border patrol device are either established and field-tested (passive optical fibre splitters~\citep{web_keysightcom}, coin flip protocols~\citep{web_filecoinio_filecoin_features_d}, hash functions+timestamps~\citep{web_knowledgedigicertcom_rfc3161_compliant_ti}\footnote{Timestamps can of course be computed after the verifier and prover have received their hashes. We do not expect nanosecond-microsecond precision to be necessary here.}) or quickly re-configurable commercial hardware: Field-Programmable Gate Arrays (FPGAs) are a suitable choice for two reasons. Firstly, they can be adapte to any specialized application after manufacturing. [source] Secondly, they are inherently more secure against supply chain attacks, since an attacker can not anticipate the circuit design in advance. For these reasons, FPGAs find use in a range of adversarial, high-stakes environments, most notably military networks ~\citep{pdf_overview_supply_chain_wp} ~\citep{web_infodascom_protection_against_s} ~\citep{pdf_infodas20sdot20securitygateway20flyer}, high-frequency trading, financial law enforcement. FPGA network cards and network taps are commercially available from multiple vendors. [sources] ~\citep{web_bittwarecom_ia_780i_fpga_acceler} These systems are deployed at scale, operate at line rate (10–400 Gb/s), introduce negligible latency, and are relied upon for regulatory compliance where evidence integrity is legally contested (e.g. Reg NMS, MiFID II [sources]). Their threat model in trading networks is explicit: traders attempt to hide behavior [source]; regulators require complete, lossless, timestamped records.
A specific example is Metamako/Arista Networks, whose 7130 FPGA taps [source] are used for precision timestamping. The German stock exchange (Deutsche Börse) publicly disclosed deployment of dozens of such devices. [source]

Lastly, re-timing devices and active wardens have been built and tested at line rates exceeding 100G.

\ul{scrubbing~\mbox{\citep{pdf_sec20_xing}}/~\mbox{\citep{web_openbsdorg_openbsd_pf_packet_f}}} traffic normalization~\citep{web_openbsdorg_openbsd_pf_packet_f}, and re-timing devices~\citep{web_epj_conferencesorg_delay_normalization_} against timing and analogue side channels.

The particular combination of these components we are proposing is novel, and we simulate their behaviour using software written in Python and available on Github~\citep{github_warden}. Our simulation works as follows:

\begin{enumerate}
\item
  A server hosts a JSON file (this is intended to replicate a datacentre hosting some output tokens stored in JSON that are intended to be sent to a user that has requested them)
\item
  The client requests the JSON file (this is the user that is expecting some output tokens from a language model)
\item
  The server sends the JSON file over either TCP or UDP in a way that makes the packet stream reproducible (this is important because it is what allows the verifier to later confirm that the traffic implies no undeclared work was performed)

  \begin{enumerate}
  \item
    Before the packets are forwarded to the client, the warden scrubs all non-reproducible components of the packets and sends them to the client
  \item
    We additionally construct the TCP and UDP packets to be deterministic, which we do by assembling the packets manually in userspace, where this would typically be done by the kernel using nondeterministic heuristics
  \end{enumerate}
\item
  We log hashes and timestamps for each packet (this simulates the passive tap)
\end{enumerate}

The verifier will later run the exact same procedure on their own hardware, comparing their traffic hashes and timestamps to confirm tha the prover's traffic implied they executed the same program as th verifier.

\section{Results}\label{results}

For both TCP~\citep{web_enwikipediaorg_transmission_control} an UDP~\citep{web_enwikipediaorg_user_datagram_protoc} traffic, we are able to detect unexpected bytes in Ethernet frames~\citep{web_enwikipediaorg_ethernet_frame_wik}. We simulate this by having the prover send additional Ethernet frames in an attempt to send the client an additional JSON file. After scrubbing the non-reproducible fields from the frames, the comparison is trivial: the verifier compares a hash of its traffic with a hash of the prover's traffic.\footnote{This is somewhat different from the live-checking of the protocol headers via the verifier's and prover's respective devices in the border patrol machine, as shown above. The security of ISNs is not broken here since their generation seeds are kept inside the warden, but not revealed until they expire.}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{media/image4.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}

The output of \href{https://github.com/derpyplops/warden/blob/main/analyze.py}{\ul{analyze.py}} when we run our simulation using UDP with steganography disabled. The traffic is identical, so the hashes match.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{media/image5.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}
The output of \href{https://github.com/derpyplops/warden/blob/main/analyze.py}{\ul{analyze.py}} when we run our simulation using UDP with steganography enable. The traffic differs, so the hashes are not equivalent.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{media/image6.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}

The output of \href{https://github.com/derpyplops/warden/blob/main/analyze.py}{\ul{analyze.py}} when we run our simulation using TCP with steganography enabled. The traffic differs, so the hashes are not equivalent.

Additionally we demonstrated an attack similar to PHY-Covert Channels~\citep{pdf_nsdi14_paper_lee} to exfiltrate the word ``SECRT'' using our standard Warden device:

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{media/image7.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}
Our steganography simulation is drastically less sophisticated than plausible real-world steganography attempts -- the prover has not made any real effort to disguise the unexpected traffic they are sending. However, it is still a demonstration of catching discrepancies in traffic using an active warden and reproducible network traffic. Further, if the traffic is entirely either reproducible or warden-generated, unless the warden is manipulated into rewriting nondeterministic fields with information the prover intends t exfiltrate \emph{and} the verifier device's observation of the warde behaviour fails, steganography is not possible at the protocol level. Therefore, we argue this is a reasonable demonstration of steganography detection.

With our proposed packet-buffering approach enabled we manage to fully divert the attack at timing frequencies of less than 1ms, constraining this attack vector to a max. theoretical bandwidth of 1kbps, while only adding minimal latency (avg. inter packet delay of 0.01ms/packet) and throughput costs (-3.4\%).

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{media/image8.png}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}
\section{Discussion and Limitations}\label{discussion-and-limitations}

\subsection{Limitations}\label{limitations}

\begin{itemize}
\item
  We only achieve a simulated bandwidth of approx. 39 Mbps, so it's not clear whether a software approach for implementing the border patrol device is feasible, possibly necessitating significant hardware design challenges Still, we presented strong precedents for the individual components, and active wardens have been built to handle 100G line rates~\citep{pdf_sec20_xing}, which would in fact be over-provisioned for most long-range internet fiber connections.
\item
  We do not constrain bandwidth for covert communication \emph{into} the datacenter. While the input data has many uncontrollable variables (media input, user prompts, \ldots), input wardens may re-write such information while preserving semantics. We leave this to future work.
\item
  There still is some potential for covert outbound communication via low-frequency timing modulation, but many orders of magnitude less than without the border patrol device.
\end{itemize}

\subsection{Future Work}\label{future-work}

\begin{itemize}
\item
  A hardware-implementation demo of our proposed setup. We expect the bill of materials of a somewhat representative device to cost less than the researcher's and engineer's time needed to build it.
\item
  Threat modeling for very low outbound bandwidth--but still impactful--covert workloads in the datacenter.
\end{itemize}

\section{Conclusion}\label{conclusion}

Firstly, a brief feasibility assessment: In the appendix B, we calculate a bandwidth of \textasciitilde40GB/s API egress out of a 100.000 GPU AI cluster streaming 200.000.000 tokens per second. The border patrol device, even under such extreme assumptions, can use off-the-shelf hardware. The storage requirements for both the prover and the verifier are insignificant for any realistic data management solution. Secondly, we see passive optical splitters as under-appreciated assets for designing verification setups under mutual distrust. Finally, we encourage further work on the hardware development to prepare border patrol devices for AI governance quickly, as they may be needed soon.

\section*{Code and Data}\label{code-and-data}

\begin{itemize}
\item
  \textbf{Code repository}: https://github.com/derpyplops/warden~\citep{github_warden}
\item
  \textbf{Interactive demo link 1:} https://warden-hackathon.vercel.app/~\citep{web_warden_hackathonvercelapp_verifying_agreements}
\item
  \textbf{Interactive demo link 2:} https://www.felix-krueckel.com/warden.html~\citep{web_felix_krueckelcom_fingerprinting_ai_cl}
\item
  \textbf{Ethernet protocol visualization:} https://ethernet-frame-visualization.vercel.app/~\citep{web_ethernet_frame_visualizationvercelapp_warden_protocol_sta}
\end{itemize}

\section*{Author Contributions}\label{author-contributions}

Naci Cankaya led the project and architecture design of the border patrol machine. Jakub Kryś led the red-teaming of the security and feasibility via thought experiments and Fermi estimates. Jonathan Ng led the creation of the simulation codebase. Felix Krückel contributed the visualizations and literature research into covert communication threats. Luke Marks contributed insights into Ethernet protocol details. All authors contributed in general literature research, fact-checking and writing.

\bibliography{main.bib}

% -------- Appendices ---------
\begin{appendices}
\crefalias{section}{appendix}

\section{Prover's Verification Mechanism}\label{appendix-a-provers-verification-mechanism}

We now consider the problem of how verification of such hashed values could take place. The fundamental challenge is as follows: given a hash value chosen by the Verifier out of the annual pool of hashes, how can the Prover reliably and efficiently demonstrate that they possess a record entry which hashes to this value\footnote{Once again, in this work we do not consider the problem of verifying that this declared workload is compliant with some regulations, only the problem of verifying that no other undeclared workloads have been run.}?

A simple idea is to maintain a traditional `hash table' -- a dictionary of all plaintext LLM inputs, the corresponding plaintext outputs and their hash values. Since the Prover already controls the egress from the data centre, they can easily register how many Ethernet frames each output gets broken into. Furthermore, since the Border Patrol protocol would be fully open-source, they also know the `hashing boundary' -- how many frames are included in one hash. This allows the Prover to have a causal association between a prompt, the plaintext output and the corresponding sequence of hashes. The hash values flow back to the Prover through a passive network tap placed on the connection which carries them from the Border Patrol to the verification cluster (see Fig. 2).

This in turn can be used for an extremely efficient hash lookup. When the Verifier specifies the hash whose provenance is to be demonstrated, the Prover finds the key associated with this hash and forwards it to the secure verification facility for replay. The facility recomputes the hash and presents it to the Verifier, who is then satisfied that the Prover must have carried out a workload that produces this hash.

The Border Patrol protocol needs to agree on the number of Ethernet frames corresponding to the fundamental `verification unit' that gets hashed. This number is arbitrary and should strike a balance between storage requirements and recomputation time. At the extreme end of the spectrum, we could hash every single Ethernet frame, leading to the highest possible number of hashes in storage. However, since in this scenario one hash corresponds to a very small number of output tokens, this minimises the number of outputs that need to be recomputed in the verification facility. On the other hand, we could hash together all Ethernet frames that pass the Border Patrol within a month, which would lead to only one hash in storage, but would require one month of recomputation during verification.

The Prover needs to maintain knowledge of which inputs and parts of the corresponding outputs were included in each hash. For example, assume that the hashing window is 150 Ethernet frames, two input prompts generate 120 and 70 tokens, respectively, and one output token is streamed out via a single Ethernet frame. The Prover must then associate the following with the hash values h1 and h2:

```\emph{\hfill\break
}\{\\
h1: {[}(prompt\_1, output\_1, 1, 120), (prompt\_2, output\_2, 1,
30){]},\\
h2: {[}(prompt\_2, output\_2, 31, 70), \ldots{]},\\
\ldots{}\\
\}
```

In this way, when the Verifier demands that hash h1 be reproduced, the Prover can immediately look up the fact that prompts 1 and 2 must be sent out to the verification facility\footnote{Note that in general, these could be batched prompts or some other type of input. We used single prompts for simplicity in this illustrative example.} (alongside the corresponding outputs and their indices). This information is necessary for the verification facility to faithfully reproduce the exact stream of 150 Ethernet frames that should be hashed and compared to h1.

\section{Storage Requirements}\label{appendix-b-storage-requirements}

We start by calculating an upper estimate of the volume of the hashes and plaintext data that would be required to be stored by the verifier and prover respectively. We assume worst-case conditions in order to demonstrate the feasibility of our proposal.

First, let us assume that the datacenter under monitoring contains 100k GPUs of the NVIDIA Hopper generation. Each such GPU is assumed to be capable of a 2000 token/s inference throughput\footnote{Deepseek's inference setup for their v3/R1 models is reported to achieve a token throughput of \href{https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md\#:~:text=\%7E14\%2E8k\%20tokens\%2Fs}{\ul{\textasciitilde14.8k tokens per H800 node}} (8 GPUs, decode unit) per second.}, meaning that the total throughput of the cluster is 2*10\^{}8 tokens/s. Next, we need to calculate how many bytes of data this token volume corresponds to. For this, we assume that the tokens are wrapped in OpenAI's streaming API template of the following format:

```
\{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
"system\_fingerprint": "fp\_44709d6fcb",
"choices":{[}\{"index":0,"delta":\{"content":"Hello"\},"logprobs":null,"finish\_reason":null\}{]}\}
```

Importantly, note that this template includes a single response token only. This is because the streaming API aims to deliver generated tokens to the user as quickly as possible, rather than waiting for the full generation to finish before sending the payload. The `one-token-per-json' scenario represents the most challenging configuration for our methodology, as this will lead to the highest possible number of hash computations.

Furthermore, we operate under an (unrealistic) assumption that each such template is allocated to one Ethernet frame. The payload is around 200 ASCII characters (see the streaming format above), corresponding to 200 bytes (+ headers) -- well below the Maximum Transmission Unit (MTU) of the Ethernet protocol.

Therefore, the total number of Ethernet frames streamed out of the cluster is simply 2*10\^{}8 per second, same as tokens. We then calculate the SHA-256 hash of each such frame, leading to 2*10\^{}8 * 24 * 3600 * 365 = 6.3*10\^{}15 hash computations performed in a year, with a corresponding hash volume of 6.3*10\^{}15 * 256 / 8 = 2*10\^{}17 bytes \textasciitilde{} 200PB.

Additionally, the Prover needs to maintain a year-long record of all plaintext data transmitted from their cluster to the outside world. Each token generated in their cluster is assumed to be wrapped in the template above. Since one ASCII character is stored by exactly 1 byte, this means that to stream one token out of the cluster, 200 bytes are required. On an annual scale, this corresponds to 2*10\^{}8 * 200 * 24 * 3600 * 365 = 1.26*10\^{}18 bytes \textasciitilde{} 1,260PB.

The cost of storing such a large amount of data is small compared to the costs of AI datacenters at the scale considered here (\textless1\%): A typical price for a 28TB HDD hard drive is around 500€\footnote{As of early 2026}, thus the full cost amounts to 1,260 PB/28TB * 500€ \textasciitilde20,000,000€.

Note that all these estimates represent a pessimistic upper bound that arises from \emph{extremely} inefficient data management. When using non-streaming APIs, multiple tokens are included as part of one chat completion template. Further, multiple such templates could be transmitted as part of a single Ethernet frame, as long as the total payload size does not exceed the standard Ethernet MTU of 1500 bytes. Storing API json files for each token is also likely unnecessary when they can be reconstructed from tokens and heuristics. We did not include data compression at all.

Finally, we do not need to compute the hash of each Ethernet frame. Instead, the `fundamental unit' of verification could be an arbitrarily long chain of frames. All these modifications would reduce storage by \textgreater3 orders of magnitude.

\section{Preventing dictionary attacks}\label{appendix-c-preventing-dictionary-attacks}

The Prover might be worried that the Verifier can deduce the plaintext content of the messages passing through the Border Patrol by guessing which content hashes to the observed hash values. Calculating this content by inverting the hash is not possible -- hash functions are by design irreversible. However, an adversarial Verifier could be tempted to use so-called dictionary attacks. In other words, they could pre-compute the hashes of the most common plausible inputs and simply compare them to the observed ones. The feasibility of this approach is entirely determined on the expected size of the input space of the hash function. For example, if there can only ever be 10 possible inputs, then it is trivial to pre-compute their 10 corresponding hashes and infer the input that has produced the hash coming out of the Border Patrol.

Unfortunately for the Prover, the input space of their API responses is indeed somewhat limited: these can only be tokens generated by the LLM\footnote{The surrounding chat completion template that encompasses the output tokens is also a part of the content that gets hashed. However, there are no degrees of freedom there. To understand why, first note that the verification cluster replays the workload using the provided inputs, inserts the output tokens into the completion template, hashes this JSON object and compares it to the observed hash from the Border Patrol. However, the replayed computation will have a different timestamp and fingerprint to the one that produced the original hash. Thus, the Prover needs to supply the verification cluster not only with the input prompt, but also the exact metadata in the output template. In other words, the Prover declares: `Here is the right prompt and the corresponding template metadata, but not the output itself'. Thus, the only degree of freedom is really limited to the output tokens.}. These in turn are constrained by the size of the LLM vocabulary, typically around 200 000 tokens. Thus, in the scenario presented above -- where only one output token is inserted into the streaming completion template, this template corresponds to one Ethernet frame and later one hash -- the attacker could pre-compute the hashes of all tokens inserted into these templates. They would then simply perform the lookup in a hash table with 200 000 values and immediately infer the right token. Thus, it would be trivial for the Verifier to guess all the tokens from the streaming API responses.

To calculate how many tokens need to be inserted into the chat template to prevent such attacks through combinatorial complexity, let us consider how many hash computations a motivated attacker could perform in a month. Specialised chips can achieve throughputs of 200TH/s (terahashes per second). Within a month, this corresponds to 5.2*10\^{}20 hashes computed. On the other hand, a sequence of \emph{n} output tokens out of a 200k vocabulary can lead to (2*10\^{}5)\^{}\emph{n} distinct inputs. Thus, the attacker is guaranteed to guess the sequence corresponding to a given hash if the sequence is shorter than \(log_{2*10^{5}}(5.2*10^{20})\ \)\textasciitilde{} 3.9 tokens long. Running multiple specialised chips in parallel will not help the attacker much -- with 1000 hash-crunching chips, the token number increases to 4.5.

Overall, to be on the safe side, the streaming API template should enclose at least 5 tokens in order to prevent brute-force hash guessing by the Verifier. However, this back-of-the-envelope calculation assumes that the attacker needs to try all possible \emph{n}-gram combinations of tokens from the vocabulary. This is a totally unrealistic assumption, as the tokens streamed out of the cluster will almost always be semantically meaningful -- sequences of tokens that form sentences, code snippets, equations, or similar. Thus, the search space is massively reduced by noting that only a small fraction of the vocabulary is likely to follow a token like `The'. Assuming that only 1000 most likely tokens need to be checked, our `effective vocabulary' is reduced to 1000. Then, the calculation becomes \(log_{2*10^{5}}(5.2*10^{20})\ \)\textasciitilde{} 7. In other words, any semantically meaningful sequence of fewer than 7 tokens could be brute-forced by the Verifier in less than a month with a single hash-crunching chip. Thus, to prevent this, the streaming API output template could wrap at least 10 tokens. This should have a negligible effect on user experience -- instead of seeing tokens appear on their screen one at a time, the user will receive them 10 at a time. Alternatively (or additionally), the hashes could simply be computed on more than one JSON template or more than one Ethernet frame. For example, if the hashing boundary is set every 100 Ethernet frames, the whole problem disappears even with 1 streaming token per JSON template.

\section{Eliminating steganography needs connection tables}\label{appendix-d-eliminating-steganography-needs-connection-tables}
The need to prevent steganographic communication between the data centre and the outside world introduces the need to eliminate certain degrees of freedom from the protocol metadata. To illustrate this more concretely with an example, in this Appendix we justify why source and destination ports need to be explicitly handled by the Active Warden.

Network protocols grant both communication parties control over certain protocol fields. In UDP, the initiating party (client) selects its source port, whilst the responding party (server) must use the destination port specified in the request as its source port for the response. Without intervention, either party could encode information through port selection.

Consider a simple example: an external client initiating API requests to a datacenter. By choosing source port 5000 versus 5001, the client transmits one bit of information per connection. At 1000 requests per second, this yields 125 Bps or \textasciitilde10 GB/day of covert channel capacity. Similarly, when the datacenter initiates outbound connections (e.g., fetching training data), it could encode information in its choice of ephemeral source port (typically ranging from 32768 to 60999, providing \textasciitilde15 bits per connection). The Active Warden must eliminate both channels.

Two separate connection tracking tables are required, because connections can be initiated from either direction:

\begin{itemize}
\item
  \textbf{External-initiated connections} (e.g. API requests to the datacenter) allow an external client to encode information in their choice of client source port. The warden must replace this with a random value, then maintain state to translate responses back to the original port.
\item
  \textbf{Datacenter-initiated connections} (e.g. fetching external datasets) allow the datacenter operator to encode information in their choice of client source port. Again, the warden must replace this with a joint random value and maintain a reverse translation state.
\end{itemize}

These cannot share a single table because the same port number (for example, 5001) might be simultaneously assigned to a datacenter client port in one connection and to an external client port in another. The tables must be keyed by the full connection 4-tuple: (source IP, source port, destination IP, destination port) in order to disambiguate these two potentially conflicting cases.

Each state-tracking table has two distinct security functions during the connection lifecycle. First, during the request phase, the table performs port randomisation: the initiating party's source port (client port) is replaced with a value from the RNG, eliminating their ability to encode information. Simultaneously, the table records the target server port from the request's destination field. During the response phase, the table performs enforcement and back-translation: it verifies the responding party uses the correct server source port (which prevents the server from encoding information through port choice), and it translates the response destination port from the RNG value back to the original client port. This ensures that the response is delivered to the correct client port and does not carry covert information through the choice of the server port.

Similar translation tables need to be maintained wherever parts of the relevant network protocol offer the attacker degrees of freedom that could be used for steganographic communication. This could introduce significant programmatic complexity for more complicated protocols like TCP, which are stateful. In TCP, the Active Warden must additionally perform sequence number translation on every single packet, and in both directions. More precisely, during the so-called `TCP handshake' the client's Initial Sequence Number (ISN) is replaced with a random number. Thus, the Warden must record the offset between the original and scrubbed ISN, so that all subsequent packets in this connection have their outbound sequence numbers incremented by this offset. Similarly, the corresponding inbound ACK numbers in this connection need to be decremented by the same offset. This is necessary to maintain consistency in both the original and scrubbed sequences. Overall, the Active Warden must track connection state to handle the handshake phase, data transfer phase, and connection termination. Each TCP connection might require around 200 bytes of state-tracking data. For a data centre with O(1000) concurrent connections processing millions of packets per second, the Active Warden must implement efficient hash lookups to avoid slowdowns in serving this traffic to customers.

\section{Enhanced design for the border patrol device}\label{appendix-e-enhanced-design-for-the-border-patrol-device}

The architecture shown below combats timing and analog modulation side-channels between the verifier and their hash processor in the border patrol device. We refer to Uttarwar et al.~\citep{web_epj_conferencesorg_delay_normalization_} for more details on the timing warden. Another enhancement here is passive observation on both sides of the active warden.

\includegraphics[width=6.35174in,height=3.80704in]{media/image6.png}

\end{appendices}

\end{document}
